# Ponderada - Semana 07
## Previs√£o de Pre√ßos de Abacate com RNN

Este projeto utiliza uma Rede Neural Recorrente (RNN), implementada com PyTorch, para prever os pre√ßos m√©dios de abacates com base em dados hist√≥ricos.

### Objetivo

Prever o pre√ßo futuro dos abacates a partir de uma sequ√™ncia de pre√ßos passados, utilizando s√©ries temporais e uma arquitetura de rede neural recorrente simples.

### Modelo

- Tipo de modelo: RNN (com `nn.RNN`)
- Biblioteca: PyTorch
- Fun√ß√£o de perda: MSELoss
- Otimizador: Adam
- Normaliza√ß√£o: `MinMaxScaler` (0 a 1)

### Estrutura dos Dados

O dataset utilizado √© o `avocado.csv`, que cont√©m informa√ß√µes de vendas de abacates nos EUA.

A principal vari√°vel usada √©:

- `AveragePrice`: pre√ßo m√©dio do abacate

### Pr√©-processamento

1. Carregamento do dataset com pandas
2. Normaliza√ß√£o da coluna `AveragePrice` entre 0 e 1
3. Cria√ß√£o de janelas (sequ√™ncias) para entrada na RNN
4. Convers√£o para tensores do PyTorch

### Arquitetura

```python
class AvocadoPriceRNN(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=1):
        super().__init__()
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        out, _ = self.rnn(x)
        out = self.fc(out[:, -1, :])
        return out
```
### Treinamento

O treinamento da RNN foi realizado para prever o pre√ßo m√©dio do abacate com base em s√©ries temporais.

#### Configura√ß√µes Principais

- **√âpocas**: 100 (ou ajust√°vel de acordo com a performance)
- **Crit√©rio de parada**: otimiza√ß√£o da fun√ß√£o de perda (MSELoss)
- **Divis√£o dos dados**: 80% para treino, 20% para teste
- **Batch size**: n√£o utilizado explicitamente (treinamento em lote completo)
- **Otimiza√ß√£o**: Adam com taxa de aprendizado padr√£o (`lr=0.001`)

#### Loop de Treinamento

```python
model = AvocadoPriceRNN()
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    output = model(X_train)
    loss = criterion(output, y_train)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 10 == 0:
        print(f'√âpoca {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')
```

### üìâ Avalia√ß√£o

Ap√≥s o treinamento, o modelo √© avaliado com os dados de teste. A performance √© comparada visualmente com os valores reais por meio de gr√°ficos.

```python
model.eval()
with torch.no_grad():
    predicted = model(X_test)
    predicted = scaler.inverse_transform(predicted.numpy())
    actual = scaler.inverse_transform(y_test.numpy())
```

### Avalia√ß√£o

Ap√≥s o treinamento, o modelo √© avaliado com os dados de teste. A performance √© comparada visualmente com os valores reais por meio de gr√°ficos.

```python
model.eval()
with torch.no_grad():
    predicted = model(X_test)
    predicted = scaler.inverse_transform(predicted.numpy())
    actual = scaler.inverse_transform(y_test.numpy())
```